## Designing Twitter Search

### 1. 什么是Twitter Search

Twitter让用户可以随时更新他们的状态。每条推文由纯文本组成，我们的目标是设计一个允许搜索所有用户推文的系统。

### 2. 需求分析

* 假设Twitter有15亿用户，每天有8亿活跃用户
* Twitter平均每天收到4亿条推文
* 一条推文的平均大小是300字节
* 假设每天会有5亿次搜索
* 搜索查询将由AND/OR组合的多个单词组成

### 3. 容量估算

存储容量：每天由4亿条新推文，平均每条推文300字节，因此需要的总存储空间为：
$$
400M * 300 \ge 120GB/day
$$
每秒总存储为:
$$
120GB /24h/3600s \approx 1.38MB/s
$$

### 4. API 设计

可以使用SOAP或者REST API来展示服务的功能。以下是搜搜API的定义：

```
search(api_dev_key, search_terms, maximum_results_to_return, sort, page_token)
```

 **Parameters**: 

* api_dev_key (string)：注册账户的API开发人员密钥，除其他用途外，这将用于根据分配的配额限制用户
* search_terms (string):  包含搜索词项的字符串
* maximum_results_to_return (number): 返回的推文数
* sorts (number):  可选排序模式:最新第一(0 -默认)，最佳匹配(1)，最喜欢(2) 
* page_token (string):   此标记将在结果集中指定应该返回的页面。 

 **Returns**: (JSON) 

 包含与搜索查询匹配的tweet列表信息的JSON。每个结果条目可以有用户ID和名称、推文、推文ID、创建时间、喜欢的数量等。 

### 5. 顶层设计

 我们需要将所有数据存储在数据库中，并建立一个索引来跟踪哪个单词出现在哪个tweet中。这个索引将帮助我们快速找到用户试图搜索的tweet。此即倒排索引。

 ![image-20191107171033874](C:\Users\xiehailian\OneDrive\xiehailian\coding-practice\go\design\twitter-search\image-20191107171033874.png)

### 6. 组件设计

#### 存储

每天要存储120GB的新数据。考虑到如此庞大的数据量，需要使用数据分区方案，以便将数据有效地分布到多个服务器，如果计划未来五年，我们将需要地空间为：
$$
120GB * 365days * 5years \approx 200TB
$$
如果不想在任何时候都有超过80%的内存，那么大约需要250TB的总存储空间。假设我们想要保留一份所有tweet的副本进行容错，那么总存储需求将是500TB。如果假设一个服务器可以存储4TB的数据，那么我们将需要125个这样的服务器来保存未来5年所有的数据。

让我们从一个简单的设计开始，将tweet存储在MySQL数据库种，假设将tweet存储在一个只包含两列（TweetID和TweetText）的表中。假设基于TweetID对数据进行分区。如果TweetID在系统范围内是唯一的，那么可以定义一个散列函数，该散列函数可以将TweetID映射到存储服务器，并在其中存储tweet对象。

那么如何创建一个全局唯一的TweetID。如果每天4亿新的推文，那么下来的5年总共有多少推文？
$$
400M * 365days * 5years \ge 730billion
$$
这意味着我们需要一个5字节的数字来唯一标识TweetID。假设一个服务可以需要存储对象时生成一个唯一的TweetID。我们可以为TweetID提供散列函数，以查找存储服务器并将tweet对象存储在那里。

#### 索引

索引应该是怎样的？因为tweet的查询将由单词组成，所以需要构建能够告诉我们哪个单词来自哪个tweet对象的索引。先估计一下索引的大小。如果我们要为所有的英语单词和一些著名的名字简历一个索引，那么索引中将会有50万个单词。假设一个单词的平均长度是5个字符。如果把索引保存在内存中，需要2.5MB内寸来存储所有的单词
$$
500K * 5 \ge 2.5MB
$$
假设仅将过去两年的所有tweet的索引保存在内存中。因为我们将在5年内收到7300亿推文，两年就是2920亿条推文。假设每个TweetID是5个字节，需要多少内存来存储所有的TweetID？
$$
292B * 5 \ge 1460GB
$$
索引就像一个大的分布式哈希表，其中key是单词，value是包含该单词的所有tweet的TweetID列表。假设每条推文40个单词，由于我们不会对借此和其他小单词如the、an、and等进行索引，假设每条推文中有15个单词需要被索引。这意味着TweetID将在索引中存储15次，所以需要存储索引的总内存是：
$$
(1460 * 15) + 2.5MB \approx 21TB
$$
假设一个服务器144GB的内存，那么需要152个服务器来保存索引。我们可以根据一下两个标准对数据进行切分：

根据单词分区。在构建索引时，遍历tweet的所有单词，并计算每个单词的散列值，已找到他们将被索引的服务器。要找到包含特定单词的所有tweet，我们只需要查询包含该单词的服务器。对于这种方法，有两个问题：

1. 如果某个词是热点词怎么办？服务器上就会有很多关于这个词的查询，高负载会影响服务性能。
2. 随着时间的推移，一些单词可能会比其他单词存储更多的TweetID。因此在tweet增长的同时，保持单词的统一分布是相当棘手的。

要从这些情况中恢复，要么重新分区数据，要门使用一致性哈希。

根据推文对象分区。在存储时，把TweetID传递给散列函数，以查找服务器并将tweet的所有单词编入索引。在查询特定单词时，必须查询所有服务器，每个服务器返回一组TweetID。集中式服务器将聚合这些结果，并返回给用户。

![image-20191109093012678](C:\Users\xiehailian\OneDrive\xiehailian\coding-practice\go\design\twitter-search\image-20191109093012678.png)

### 7. 容错设计

当索引服务器宕机时会发生什么？可以使用每个服务器的第二个副本，如果主服务器宕机，它可以在故障转移后接管控制权。主服务器和辅助服务器都具有相同的索引副本。

如果主服务器和辅助服务器同宕机怎么办？必须分一个新的服务器并在其上重建相同的索引。我们该怎么做？我们不知道在这个服务器上保存了哪些单词/推文。如果使用基于推文对象的分片，解决方案是遍历这个歌数据库，并使用散列函数过滤TweetID，以找出将存储在此服务器上的所有必须tweet。这是抵消的，而且在服务器重建期间，无法从它提供任何查询，因此一些用户可能看不到一些推文。

如何有效地检索tweet和索引之间的映射？我们必须构建一个倒排索引，将所有的TweetID映射到他们的索引服务器。Index-Builder服务器可以保存这些信息。我们需要构建一个Hashtable，其中key是索引服务器编号，value是一个HashSet，包含了保存在该索引服务器上的所有TweetID。注意将所有的TweetID保存在集合里，可以使我们能够快速的添加/删除tweet。因此，无论合适索引服务器需要重建，都可以简单的请求Index-Builder服务器为他需要存储的所有推文提供数据，然后获取这些推文构建索引。这种方法肯定很快，我们还需要一个Index-Builder服务器的副本，用于容错。

### 8. 缓存

为了处理热点推文，我们可以在数据库前面加一个缓存，可以使用Memcached，可以将所有热门推文存储在内存中。应用服务器在访问后端数据库之前，可以快速检查缓存是否有该推文。根据客户端的使用模式，可以调整需要多少缓存服务器。对于缓存失效策略，LRU适合我们的系统

### 9. 负载均衡

可以在系统的两个地方添加负载均衡层：

1. 客户端和应用服务器之间
2. 应用服务器和后端服务器之间

最初可以采用简单的轮询算法，在后福安服务器之间平均分配传入请求。这个LB实现起来很简单，不会带来任何开销。这种方法的另一个好处使LB将停止向不再服务的服务器发送任何流量。它存在的问题使没有考虑到服务器的负载。如果服务器超载或速度变慢，LB将不会停止向该服务器发送新的请求。为了处理这个问题，可以使用更智能的LB解决方案，定期查询后端服务器的负载，并根据负载调整流量。

### 10. 排序

如果我们想要根据社交图距离、流行度、相关性等对搜索结果进行排序，又该如何呢？假设我们想要根据受欢迎程度对tweet进行排序，比如一条推文获得了多个赞或评论等等。在这种情况下，排名算法可以计算一个受欢迎度，并将其存储到索引中。在将结果返回到聚合服务器之前，每个分区都可以根据此流行度编号对结果进行排序。聚合服务器将所以这些结果组合在一起，根据流行度编号对它们进行排序，并将最终结果返回给用户。





