## Designing Twitter

### 1. Twitter 是什么?

Twitter是一个在线的社交网络服务，用户可以在上面发布和阅读140字的短消息，称为tweets。注册用户可以发布和阅读tweets，但未注册的用户只能阅读tweets。用户通过网站页面、短信或移动应用程序访问Twitter。

### 2. 需求分析

#### 功能性需求

1. 用户可以发送新的tweets
2. 一个用户可以关注其他用户
3. 用户可以收藏tweets
4. 这个服务应该能创建和显示用户的时间轴，该时间轴由用户所关注的所有人的最新twieets组成
5. tweets可以包含照片和视频

#### 非功能性需求

1. 服务必须高可用
2. 时间线生成的可接受延迟是200ms
3. 一致性可能会受影响（为了满足可用性）。如果有暂时没有看到tweet，这是正常的现象。

#### 额外需求

1. tweets的搜索
2. 回复一条tweet
3. 热门话题，当前活跃的主题和搜索
4. 给别的用户打标签
5. tweet通知
6. 关注谁？建议？
7. 动态

### 3. 存储估算

假设总共有10亿用户，每天2亿活跃用户（DAU）。同时假设每天有1亿新的tweets，平均每个用户关注200人。

每天有多少收藏？如果平均每个用户每天收藏5个tweet，那么：
$$
200000000 * 5 = 1000000000
$$
系统总工会产生多少的推文浏览量？假设一个用户平均每天访问他们的时间线两次，并访问其他五个人的页面，如果用户在每个页面上看到20条tweet，那么没有tweet的总浏览量为280亿：
$$
200000000 * ((2 + 5) * 20) = 28000000000
$$
存储估算。假设每个tweet有140个字符，在不压缩的情况下，需要两个字节来存储一个字符。假设需要30个字节来存储每个tweet的元数据（如ID、时间戳、用户ID等）。需要的总存储空间为30GB：
$$
100000000 * （280 + 30） = 30
$$
五年的存储需求是多少？需要多少存储空间来存放用户的数据、关注和收藏？并不是所有的推文都有视频，假设平均每5条推文就有一张照片，每10条推文就有一个视频，照片的平均大小是200KB，视频的大小是2MB。也就是说每天会有24TB的新内容：
$$
(100M/5 *200KB) + (100M/10 * 2MB) \approx 24TB/day
$$
带宽估算。由于每天的入口数据量是24TB，这将转化为290MB/s。每天有有280万的推文浏览量，而且必须显示每个tweet的照片（如果它有照片的话），假设用户在他们的时间线上能看到的三个视频，那么总出口为：
$$
(28B * 280bytes)/86400 + (28B/5*200KB)/86400 + (28B/10/3 * 2MB) /86400 \approx 35GB/s
$$

### 4. API 设计

可以使用SOAP或者REST API来展示服务的功能。以下是发布新推文API的定义：

```
tweet(api_dev_key, tweet_data, tweet_location, user_location, media_ids, 
maximum_results_to_return)
```

 **Parameters**: 

* api_dev_key (string)：注册账户的API开发人员密钥，除其他用途外，这将用于根据分配的配额限制用户
*  tweet_data (string):  tweet的文本，通常最多140个字符
*  tweet_location (string): tweet引用的可选的位置（经度，纬度）
*  user_location (string): 添加tweet的用户的可选位置（经度，纬度）
*  media_ids (number[]):  可选的与Tweet关联的media_id列表。(所有媒体照片、视频等需单独上传) 

 **Returns**: (string) 

成功时，返回访问该tweet的URL。否则，将返回一个适当的HTTP错误

### 5. 顶层设计

我们需要一个能有效存储所有新推文的系统，需要支持每秒发布1150篇推文，每秒32500次浏览。从需求可以清楚的看出，这将是一个包含大量读操作的系统。

在顶层设计上，需要多个应用服务器来为这些请求提供服务，这些请求的前面时用于流量分发的负载均衡器。在后台，需要一个高效的数据库，可以存储所有的新tweet，并支持大量的读取。我们还需要一些文件服务器来存储照片和视频。

![image-20191106161100161](C:\Users\xiehailian\OneDrive\xiehailian\coding-practice\go\design\twitter\image-20191106161100161.png)

尽管预计每天的写负载是1亿条推文，读负载是280亿条推文。这意味着系统平均每秒接收到大约1160条推文和325000次浏览请求。尽管如此，这些流量在一天中分布得并不均匀，高峰期预计至少会有几千次写请求和1百万次读请求。在设计系统架构时，应该谨记。

### 6. 数据库设计

需要存储四张表，推文表，用户表，用户关注表，用户收藏表。

![image-20191106162043906](C:\Users\xiehailian\OneDrive\xiehailian\coding-practice\go\design\twitter\image-20191106162043906.png)

### 7. 数据分片

由于每天都有大量新的tweet，并且读负载也非常高，需要将数据分布到多台机器上，以便能够有效地读取/写入数据。分割数据地方式有很多种。

#### 基于UserID的分区

我们可以尝试将用户的所有数据存储在一台服务器上。在存储时，可以将UserID传递给散列函数，散列函数将用户映射到数据库服务器，在哪里存储拥有所有的tweet、收藏夹、关注等。在查询用户的tweets/fllowing/favorites时，可以先通过散列函数找到用户的数据库服务器，然后从那里读取数据。这种方法有两个问题：

1. 如果用户变得很热门怎么办？在保存用户的服务器上可能有很多查询，高负载会影响服务性能
2. 随着时间的推移，与其他用户相比，一些用户可能会存储大量tweet或拥有大量关注。保持用户数据增长的均匀分布是相当困难的

为了解决这些问题，我们必须重新分区/重新分发数据，或者使用一致性哈希

#### 基于TweetID的分区

散列函数将每个TweetID映射到一个随机的服务器，我们将在其中存储Tweet。要搜索tweet，必须查询所有服务器，每个服务器返回一组tweet。集中式服务器将聚合这些结果，并将它们返回给用户。让我们看看时间线生成的例子，下面是系统生成用户的一条时间线的例子：

1. 应用服务器会找到用户关注的所有人
2. 应用服务器会将查询发送到所有数据库服务器，以查找来自这些人的tweets
3. 每个数据库服务器将为每个用户查找tweet，按最近排序返回
4. 应用服务器会合并所有结果，并再次排序，将最上面的结果返回用户

这种方法解决了热门用户的问题，但是与基于UserID分片不同，必须查询用户tweet的所有数据库分区，这回导致更高的延迟。我们可以进一步提高性能，引入缓存来在数据库服务器前缓存热点推文

#### 基于Tweet创建的时间分片

基于创建时间存储Tweet将为我们提供快速获取所有最新tweet的又是，我们只需要查询非常小的一组服务器。这里的问题是流量负载不会被分配。例如，在写入时，所有新的tweet都将发送都一个服务器，其余服务器出于空闲状态。类似地，在读取时，与持有旧数据的服务器相比，持有最新数据的服务器将具有非常高的负载。

如果我们可以通过TweetID和Tweet创建时间来合并分片呢？如果不单独存储tweet创建时间只使用TweetID来反映这一点。通过这种方式，你可以很快的找到最新的推文。为此，必须使每个TweetID在系统中是唯一的，并且每个TweetID也应该包含时间戳

我们可以使用时间戳。假设TweetID有两部分，第一部分是表示时间戳，第二部是一个自增序列。因此，要创建一个新的TweetID，可以使用当前时间错拼接一个自动递增的数字，通过这个TweetID找到分片存储在哪里。TweetID应该有多大？假设时间戳从今天开始，需要多少位来存储未来50年的秒数？
$$
86400 * 365 * 50 \ge 1.6B
$$
因此需要31位来存储这个数字。因为期望每秒写入1150条tweet，所以我们可以分配17位来存储自动递增序列。TweetID既有48位了。所以，每一秒都可以存储130000条新推文。我们可以每秒重置一次自动递增序列，为了容错和更好的性能，可以让两个数据库服务器为我们盛恒自动递增的键，一个生成偶数键，另一个生成奇数键。

如果TweetID有64位（8字节）长，可以很容易地存储下一个100年地tweet，并且以毫秒级的粒度存储它们。在上面的方法中，我们仍需要查询所有服务器来生成时间轴，但是读写都将达达加快。

1. 由于没有二级索引，这将减少写延迟
2. 读取时，不需要过滤创建时间，因为主键已经包含了时间戳

### 8. 缓存

可以位数据库服务器引入缓存来缓存热点tweet和用户。我们可以用现成的解决方案如Memcache来存储所有的tweet对象。应用服务器，在命中数据库前，可以快速的检查缓存是否有马努需要的tweets。根据客户端的使用模式我们可以决定需要多少缓存服务器。

哪种缓存更新策略最适合？当缓存已满，我们希望用更新或者更热门的tweet来替换一条tweet时，我们将如何选择？对于我们的系统来说，最近最少使用策略是一种合理策略。根据这一策略，将首先删除最近浏览次数最少的tweet。

![image-20191106212426620](C:\Users\xiehailian\OneDrive\xiehailian\coding-practice\go\design\twitter\image-20191106212426620.png)有没有更智能的缓存？如果我们用28法则，那就是20%的推文产生了80%的阅读量，这意味着某些推文非常受欢迎，可以尝试缓存每个分片每日阅读量的20%。

如果缓存最新的数据呢？服务可以从这种方法中受益。假设80%的用户只看过去三天的tweet，我们可以缓存过去三天所有的tweet。假设缓存服务器缓存了来自过去三天所有用户的所有tweet。正如前面估算的，每天会受到1亿条新推文即30GB的新数据。如果想要缓存过去三天的所有tweet，将需要不少于100GB的内存。这些数据可以很容易地放入一个服务器，但是我们应该复制到多个服务器上，以分发所有读取流量，从而减少缓存服务器上的负载。因此当我们生成用户时间轴时，我们可以询问缓存服务器时候有该用户的所有最新的tweet。如果是，我们可以简单的从缓存返回所有数。如果缓存中没有足够的tweet，就必须查询后端服务器来获取数据。在类似的设计中，我们可以尝试缓存过去三天的照片和视频。

我们的缓存就像一个哈希表，其中key是OwnerID，value是一个双链表，包含该用户在过去三天内发出的所有tweet。因为我们希望首先检索最近的数据，所以我们总是可以在链表顶部插入新的tweet，这意味着较老的tweet都将位于链表尾部附近。因此，我们可以从尾部删除tweet，为更新的tweet腾出空间。

### 9. 时间线

时间线的生成的细节参考Facebook。

### 10. 复制和容错

因为系统是大量读取的，所有每个DB分区可以有多个从数据库服务器。从服务仅用于读取流量，所有的写操作都将首先到达主服务器，然后被复制到从服务器。该方案还提供了容错性，因为无论何时主服务器宕机，都可以故障转移到辅助服务器

### 11. 负载均衡

可以在系统的三个位置添加负载均衡层：

1. 客户机与应用服务器之间
2. 应用服务器与数据库服务器之间
3. 聚合服务器与缓存服务器之间

最初，可以使用简单的轮询调度方法。它可以在服务期间平均分配传入请求。这种LB实现起来很简单，不会带来任何开销。这种方法的另一个好处是，如果服务器已宕机，LB将停止向其发送任何流量。轮询调度算法的一个问题是它不考虑服务器的负载，如果服务器过载或速度变慢，LB不会停止向该服务器发送新请求。为了处理这个问题，可以放置一个更只能的LB解决方案，它可以定期查询后端服务器的负载并根据负载调整流量。

### 12. 监控

对系统的监控是非常重要的。我们应该不断地收集数据，以便立即了解系统的运行情况。我们可以收集以下指标/计数来聊直接服务的性能：

1. 每天/每秒产生多少新推文，每天的峰值是多少
2. 时间线的交付统计，每天/每秒服务交付多少条推文
3. 用户刷新时间线的平均延迟

通过监视这些计数器，我们将了解是否需要更多的副本、负载均衡或缓存

### 13. 额外需求

如何提供信息流？从关注的哪里获取所有最新的推文，并按时间进行合并排序。使用分页来获取/显示tweet。只获取所有关注者前N个Tweet。N取决于客户端的窗口，因为移动端显示的tweets比客户端少。还可以缓存下一个热门tweet来加快速度。另外，可以预先生成动态，以提高效率

Retweet，对于数据库种的每个tweet对象，可以存储原始Tweet的ID，而不存储此Retweet对象上的任何内容

热门主题：可以在最后N秒内缓存出现最频繁的搜索查询，并在每M秒后更新它们，可以根据推门、搜索查询、转发或点赞的频率对热门话题进行牌型。可以给主题更多的权重，显示给更多的人。

要关注谁？如何推荐？这个特性可以提高用户的参与度。我们可以推荐某人的朋友关注的。

动态：通过ML监督学习或者聚类，获取过去1-2小时内获取不同网站的头条新闻，找出相关的tweet，对其进行优先级排序，并将其分类（新闻、支持、金融、娱乐等）。然后我们可以在动态里显示这些文章的热门话题

搜索：搜索包括对推文进行索引、排序和检索。





